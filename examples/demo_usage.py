"""
MiniLLM with Flash Attention and KV Cache - ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨å®é™…åº”ç”¨ä¸­ä½¿ç”¨æ•´åˆåçš„åŠŸèƒ½
"""
import torch
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from models.mini_llm import MiniLLM
from cache.kv_cache import KVCache
from config import Configs

class TextGenerator:
    """åŸºäºMiniLLMçš„æ–‡æœ¬ç”Ÿæˆå™¨"""
    
    def __init__(self, config=None, use_flash_attention=True):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            config: æ¨¡å‹é…ç½®ï¼Œé»˜è®¤ä½¿ç”¨smallé…ç½®
            use_flash_attention: æ˜¯å¦ä½¿ç”¨Flash Attention
        """
        if config is None:
            config = Configs.small()
        
        self.config = config
        self.model = MiniLLM(config)
        self.model.eval()
        self.use_flash_attention = use_flash_attention
        
        print(f"å·²åŠ è½½æ¨¡å‹:")
        print(f"  è®¾å¤‡: {config.device}")
        print(f"  ç»´åº¦: {config.dim_model}")
        print(f"  å¤´æ•°: {config.num_heads}")
        print(f"  å±‚æ•°: {config.num_layers}")
        print(f"  Flash Attention: {'å¯ç”¨' if use_flash_attention else 'ç¦ç”¨'}")
    
    def generate(self, prompt_ids, max_new_tokens=50, temperature=1.0, do_sample=True):
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt_ids: [batch_size, prompt_len] promptçš„token IDs
            max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            do_sample: æ˜¯å¦éšæœºé‡‡æ ·
            
        Returns:
            generated_ids: [batch_size, prompt_len + generated_len] ç”Ÿæˆçš„token IDs
        """
        batch_size, prompt_len = prompt_ids.shape
        device = prompt_ids.device
        
        # åˆ›å»ºKV Cache
        kv_cache = KVCache(self.config, batch_size=batch_size)
        
        print(f"å¼€å§‹ç”Ÿæˆ:")
        print(f"  Prompté•¿åº¦: {prompt_len}")
        print(f"  æœ€å¤§ç”Ÿæˆtoken: {max_new_tokens}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # å¤„ç†prompt
        with torch.no_grad():
            logits = self.model(
                prompt_ids, 
                kv_cache=kv_cache, 
                use_flash_attention=self.use_flash_attention
            )
        
        # å­˜å‚¨æ‰€æœ‰ç”Ÿæˆçš„token
        all_tokens = [prompt_ids]
        
        # é€ä¸ªç”Ÿæˆtoken
        for step in range(max_new_tokens):
            # ä»æœ€åä¸€ä¸ªä½ç½®çš„logitsä¸­é‡‡æ ·
            next_token_logits = logits[:, -1, :] / temperature
            
            if do_sample:
                # éšæœºé‡‡æ ·
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
            else:
                # è´ªå¿ƒè§£ç 
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            
            all_tokens.append(next_token)
            
            # è·å–ä¸‹ä¸€ä¸ªtokençš„logitsï¼ˆç”¨äºä¸‹ä¸€æ­¥ç”Ÿæˆï¼‰
            with torch.no_grad():
                logits = self.model(
                    next_token,
                    kv_cache=kv_cache,
                    use_flash_attention=self.use_flash_attention
                )
            
            # æ¯10æ­¥æ‰“å°è¿›åº¦
            if (step + 1) % 10 == 0 or step < 5:
                print(f"  ç”Ÿæˆè¿›åº¦: {step + 1}/{max_new_tokens}")
        
        # æ‹¼æ¥æ‰€æœ‰token
        generated_ids = torch.cat(all_tokens, dim=1)
        
        print(f"ç”Ÿæˆå®Œæˆ! æ€»é•¿åº¦: {generated_ids.shape[1]}")
        return generated_ids
    
    def batch_inference(self, input_ids_list):
        """
        æ‰¹é‡æ¨ç†ï¼ˆéç”Ÿæˆä»»åŠ¡ï¼‰
        
        Args:
            input_ids_list: list of [seq_len] å¤šä¸ªåºåˆ—çš„token IDs
            
        Returns:
            outputs: list of [seq_len, vocab_size] æ¯ä¸ªåºåˆ—çš„logits
        """
        # å¡«å……åˆ°ç›¸åŒé•¿åº¦
        max_len = max(len(ids) for ids in input_ids_list)
        batch_size = len(input_ids_list)
        
        padded_inputs = torch.zeros(batch_size, max_len, dtype=torch.long, device=self.config.device)
        
        for i, ids in enumerate(input_ids_list):
            padded_inputs[i, :len(ids)] = torch.tensor(ids, device=self.config.device)
        
        print(f"æ‰¹é‡æ¨ç†: {batch_size}ä¸ªåºåˆ—ï¼Œæœ€å¤§é•¿åº¦{max_len}")
        
        with torch.no_grad():
            outputs = self.model(
                padded_inputs,
                kv_cache=None,  # æ‰¹é‡æ¨ç†é€šå¸¸ä¸ä½¿ç”¨KV cache
                use_flash_attention=self.use_flash_attention
            )
        
        # åˆ†å‰²å›åŸå§‹é•¿åº¦
        result_outputs = []
        for i, ids in enumerate(input_ids_list):
            result_outputs.append(outputs[i, :len(ids), :])
        
        return result_outputs


def demo_text_generation():
    """æ¼”ç¤ºæ–‡æœ¬ç”ŸæˆåŠŸèƒ½"""
    print("=" * 60)
    print("æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = TextGenerator(use_flash_attention=True)
    
    # æ¨¡æ‹Ÿprompt
    batch_size = 2
    prompt_len = 20
    vocab_size = generator.config.vocab_size
    
    # éšæœºç”Ÿæˆprompt (å®é™…ä½¿ç”¨ä¸­è¿™é‡Œæ˜¯ç¼–ç åçš„æ–‡æœ¬)
    prompt_ids = torch.randint(0, vocab_size, (batch_size, prompt_len), 
                              device=generator.config.device)
    
    print(f"Prompt IDså½¢çŠ¶: {prompt_ids.shape}")
    
    # ç”Ÿæˆæ–‡æœ¬
    generated_ids = generator.generate(
        prompt_ids=prompt_ids,
        max_new_tokens=30,
        temperature=0.8,
        do_sample=True
    )
    
    print(f"ç”Ÿæˆç»“æœå½¢çŠ¶: {generated_ids.shape}")
    print("ç”Ÿæˆå®Œæˆ!")
    
    return generated_ids


def demo_batch_inference():
    """æ¼”ç¤ºæ‰¹é‡æ¨ç†åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æ‰¹é‡æ¨ç†æ¼”ç¤º")
    print("=" * 60)
    
    generator = TextGenerator(use_flash_attention=True)
    
    # æ¨¡æ‹Ÿä¸åŒé•¿åº¦çš„è¾“å…¥åºåˆ—
    sequences = [
        list(range(10, 25)),     # é•¿åº¦15
        list(range(20, 40)),     # é•¿åº¦20
        list(range(5, 35)),      # é•¿åº¦30
    ]
    
    print("è¾“å…¥åºåˆ—:")
    for i, seq in enumerate(sequences):
        print(f"  åºåˆ—{i+1}: é•¿åº¦={len(seq)}")
    
    # æ‰¹é‡æ¨ç†
    outputs = generator.batch_inference(sequences)
    
    print("æ¨ç†ç»“æœ:")
    for i, output in enumerate(outputs):
        print(f"  åºåˆ—{i+1} è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    return outputs


def performance_demo():
    """æ€§èƒ½å¯¹æ¯”æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("æ€§èƒ½å¯¹æ¯”æ¼”ç¤º")
    print("=" * 60)
    
    config = Configs.small()
    
    # Flash Attention vs æ ‡å‡†æ³¨æ„åŠ›
    print("æµ‹è¯•Flash Attention vs æ ‡å‡†æ³¨æ„åŠ›:")
    
    flash_generator = TextGenerator(config, use_flash_attention=True)
    standard_generator = TextGenerator(config, use_flash_attention=False)
    
    # æµ‹è¯•æ•°æ®
    batch_size = 2
    prompt_len = 32
    prompt_ids = torch.randint(0, config.vocab_size, (batch_size, prompt_len), 
                              device=config.device)
    
    import time
    
    # Flash Attentionç”Ÿæˆ
    print("\nFlash Attentionç”Ÿæˆ:")
    start_time = time.time()
    flash_result = flash_generator.generate(prompt_ids, max_new_tokens=20)
    flash_time = time.time() - start_time
    print(f"ç”¨æ—¶: {flash_time:.4f}s")
    
    # æ ‡å‡†æ³¨æ„åŠ›ç”Ÿæˆ
    print("\næ ‡å‡†æ³¨æ„åŠ›ç”Ÿæˆ:")
    start_time = time.time() 
    standard_result = standard_generator.generate(prompt_ids, max_new_tokens=20)
    standard_time = time.time() - start_time
    print(f"ç”¨æ—¶: {standard_time:.4f}s")
    
    # å¯¹æ¯”
    print(f"\næ€§èƒ½å¯¹æ¯”:")
    print(f"Flash Attentionç”¨æ—¶: {flash_time:.4f}s")
    print(f"æ ‡å‡†æ³¨æ„åŠ›ç”¨æ—¶: {standard_time:.4f}s")
    if flash_time < standard_time:
        print(f"Flash Attentionæ›´å¿« {standard_time/flash_time:.2f}x")
    else:
        print(f"æ ‡å‡†æ³¨æ„åŠ›æ›´å¿« {flash_time/standard_time:.2f}x")


if __name__ == "__main__":
    print("MiniLLM Flash Attention + KV Cache ä½¿ç”¨ç¤ºä¾‹")
    
    try:
        # æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆ
        demo_text_generation()
        
        # æ¼”ç¤ºæ‰¹é‡æ¨ç†
        demo_batch_inference()
        
        # æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”
        performance_demo()
        
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
        print("ğŸš€ MiniLLMå·²æˆåŠŸæ•´åˆFlash Attentionå’ŒKV Cache")
        print("ğŸ“ å¯ä»¥ç”¨äºå®é™…çš„æ–‡æœ¬ç”Ÿæˆå’Œæ¨ç†ä»»åŠ¡")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
