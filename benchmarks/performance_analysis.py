"""
æ€§èƒ½å¯¹æ¯”å’Œä¼˜åŒ–å»ºè®®
Flash Attention + KV Cache æ•´åˆæ•ˆæœåˆ†æ
"""
import torch
import time
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from models.mini_llm import MiniLLM
from cache.kv_cache import KVCache
from config import Configs

def performance_comparison():
    """è¯¦ç»†æ€§èƒ½å¯¹æ¯”åˆ†æ"""
    print("Flash Attention + KV Cache æ€§èƒ½åˆ†æ")
    print("=" * 60)
    
    config = Configs.small()
    config.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦
    seq_lengths = [64, 128, 256, 512]
    batch_size = 2
    
    results = {
        'seq_len': [],
        'standard_time': [],
        'flash_time': [], 
        'cache_time': [],
        'standard_memory': [],
        'flash_memory': []
    }
    
    model = MiniLLM(config)
    model.eval()
    
    for seq_len in seq_lengths:
        print(f"\næµ‹è¯•åºåˆ—é•¿åº¦: {seq_len}")
        print("-" * 30)
        
        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len), device=config.device)
        
        # æ¸…ç©ºGPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # 1. æ ‡å‡†æ³¨æ„åŠ›
        with torch.no_grad():
            start_time = time.time()
            _ = model(input_ids, kv_cache=None, use_flash_attention=False)
            standard_time = time.time() - start_time
        
        # 2. Flash Attention 
        with torch.no_grad():
            start_time = time.time()
            _ = model(input_ids, kv_cache=None, use_flash_attention=True, flash_block_size=128)
            flash_time = time.time() - start_time
        
        # 3. KV Cacheç”Ÿæˆæ¨¡æ‹Ÿ
        kv_cache = KVCache(config, batch_size=batch_size)
        chunk_size = 16
        cache_total_time = 0
        
        for i in range(0, seq_len, chunk_size):
            chunk = input_ids[:, i:i+chunk_size]
            with torch.no_grad():
                start_time = time.time()
                _ = model(chunk, kv_cache=kv_cache, use_flash_attention=True)
                cache_total_time += time.time() - start_time
        
        # å†…å­˜ä¼°ç®— (ç†è®ºå€¼)
        standard_memory_mb = (seq_len ** 2 * batch_size * config.num_heads * 4) / (1024**2)  # æ³¨æ„åŠ›çŸ©é˜µ
        flash_memory_mb = (128 ** 2 * batch_size * config.num_heads * 4) / (1024**2)  # Flashå—
        
        results['seq_len'].append(seq_len)
        results['standard_time'].append(standard_time)
        results['flash_time'].append(flash_time)
        results['cache_time'].append(cache_total_time)
        results['standard_memory'].append(standard_memory_mb)
        results['flash_memory'].append(flash_memory_mb)
        
        print(f"æ ‡å‡†æ³¨æ„åŠ›: {standard_time:.4f}s")
        print(f"Flash Attention: {flash_time:.4f}s")
        print(f"KV Cacheç”Ÿæˆ: {cache_total_time:.4f}s")
        print(f"ç†è®ºå†…å­˜èŠ‚çœ: {(1 - flash_memory_mb/standard_memory_mb)*100:.1f}%")
        print(f"Speed up vs æ ‡å‡†: {standard_time/flash_time:.2f}x")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("æ€§èƒ½æ€»ç»“æŠ¥å‘Š")
    print("=" * 60)
    
    for i, seq_len in enumerate(seq_lengths):
        speedup_flash = results['standard_time'][i] / results['flash_time'][i]
        memory_savings = (1 - results['flash_memory'][i] / results['standard_memory'][i]) * 100
        
        print(f"åºåˆ—é•¿åº¦ {seq_len}:")
        print(f"  Flash Attentioné€Ÿåº¦æå‡: {speedup_flash:.2f}x")
        print(f"  å†…å­˜èŠ‚çœ: {memory_savings:.1f}%")
        print(f"  KV Cacheå¢é‡æ¨ç†æ—¶é—´: {results['cache_time'][i]:.4f}s")
    
    return results


def generation_benchmark():
    """æ–‡æœ¬ç”ŸæˆåŸºå‡†æµ‹è¯•"""
    print("\n\næ–‡æœ¬ç”Ÿæˆæ€§èƒ½åŸºå‡†")
    print("=" * 60)
    
    config = Configs.small()
    config.device = "cuda" if torch.cuda.is_available() else "cpu"
    
    model = MiniLLM(config)
    model.eval()
    
    # æµ‹è¯•ä¸åŒçš„ç”Ÿæˆé•¿åº¦
    prompt_len = 32
    generation_lengths = [32, 64, 128, 256]
    
    print(f"Prompté•¿åº¦: {prompt_len}")
    print(f"æµ‹è¯•ç”Ÿæˆé•¿åº¦: {generation_lengths}")
    
    for max_new_tokens in generation_lengths:
        print(f"\nç”Ÿæˆ {max_new_tokens} tokens:")
        print("-" * 30)
        
        # å‡†å¤‡æ•°æ®
        batch_size = 1
        prompt_ids = torch.randint(0, config.vocab_size, (batch_size, prompt_len), device=config.device)
        
        # æ–¹æ³•1: æ— KV Cache (æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—å…¨éƒ¨)
        print("æ–¹æ³•1: æ— KV Cache (é‡å¤è®¡ç®—)")
        total_time_no_cache = 0
        current_seq = prompt_ids
        
        for step in range(max_new_tokens):
            next_token = torch.randint(0, config.vocab_size, (batch_size, 1), device=config.device)
            current_seq = torch.cat([current_seq, next_token], dim=1)
            
            with torch.no_grad():
                start_time = time.time()
                _ = model(current_seq, kv_cache=None, use_flash_attention=True)
                total_time_no_cache += time.time() - start_time
        
        print(f"  æ€»æ—¶é—´: {total_time_no_cache:.4f}s")
        print(f"  é€Ÿåº¦: {max_new_tokens/total_time_no_cache:.2f} tokens/s")
        
        # æ–¹æ³•2: ä½¿ç”¨KV Cache
        print("æ–¹æ³•2: KV Cache (å¢é‡è®¡ç®—)")
        kv_cache = KVCache(config, batch_size=batch_size)
        
        # å¤„ç†prompt
        with torch.no_grad():
            start_time = time.time()
            _ = model(prompt_ids, kv_cache=kv_cache, use_flash_attention=True)
            prompt_time = time.time() - start_time
        
        # é€ä¸ªç”Ÿæˆtoken
        generation_time = 0
        for step in range(max_new_tokens):
            next_token = torch.randint(0, config.vocab_size, (batch_size, 1), device=config.device)
            
            with torch.no_grad():
                start_time = time.time()
                _ = model(next_token, kv_cache=kv_cache, use_flash_attention=True)
                generation_time += time.time() - start_time
        
        total_time_with_cache = prompt_time + generation_time
        
        print(f"  Promptæ—¶é—´: {prompt_time:.4f}s")
        print(f"  ç”Ÿæˆæ—¶é—´: {generation_time:.4f}s") 
        print(f"  æ€»æ—¶é—´: {total_time_with_cache:.4f}s")
        print(f"  é€Ÿåº¦: {max_new_tokens/generation_time:.2f} tokens/s")
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = total_time_no_cache / total_time_with_cache
        print(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
    return True


def memory_analysis():
    """å†…å­˜ä½¿ç”¨åˆ†æ"""
    print("\n\nå†…å­˜ä½¿ç”¨åˆ†æ")
    print("=" * 60)
    
    config = Configs.small()
    
    seq_lengths = [128, 256, 512, 1024, 2048]
    
    print("ç†è®ºå†…å­˜ä½¿ç”¨å¯¹æ¯” (MB):")
    print("åºåˆ—é•¿åº¦\tæ ‡å‡†æ³¨æ„åŠ›\tFlash Attention\tèŠ‚çœ")
    print("-" * 50)
    
    for seq_len in seq_lengths:
        # æ³¨æ„åŠ›çŸ©é˜µå†…å­˜: batch_size * num_heads * seq_len^2 * 4 bytes
        standard_memory = (2 * config.num_heads * seq_len * seq_len * 4) / (1024**2)
        
        # Flash Attentionå—å†…å­˜: batch_size * num_heads * block_size^2 * 4 bytes  
        block_size = 128
        flash_memory = (2 * config.num_heads * block_size * block_size * 4) / (1024**2)
        
        savings = (1 - flash_memory / standard_memory) * 100
        
        print(f"{seq_len}\t\t{standard_memory:.2f}\t\t{flash_memory:.2f}\t\t{savings:.1f}%")
    
    # KV Cacheå†…å­˜
    print(f"\nKV Cacheå†…å­˜ä½¿ç”¨:")
    kv_memory = (config.num_layers * 2 * config.max_batch_size * config.num_heads * 
                config.max_kv_cache_len * config.head_dim * 2) / (1024**2)  # 2 bytes for fp16
    print(f"KV Cacheæ€»å†…å­˜: {kv_memory:.2f} MB")
    print(f"æ”¯æŒæœ€å¤§åºåˆ—é•¿åº¦: {config.max_kv_cache_len}")
    print(f"æ”¯æŒæœ€å¤§æ‰¹æ¬¡å¤§å°: {config.max_batch_size}")
    
    return True


def optimization_recommendations():
    """ä¼˜åŒ–å»ºè®®"""
    print("\n\nä¼˜åŒ–å»ºè®®")
    print("=" * 60)
    
    recommendations = [
        "1. Flash Attentionå—å¤§å°ä¼˜åŒ–:",
        "   - çŸ­åºåˆ—(< 256): ä½¿ç”¨å—å¤§å°64-128",
        "   - é•¿åºåˆ—(> 256): ä½¿ç”¨å—å¤§å°128-256", 
        "   - æ ¹æ®GPUå†…å­˜è°ƒæ•´å—å¤§å°",
        "",
        "2. KV Cacheç­–ç•¥:",
        "   - ç”Ÿæˆä»»åŠ¡: å¿…é¡»ä½¿ç”¨KV Cache",
        "   - æ‰¹å¤„ç†æ¨ç†: æ ¹æ®åºåˆ—é•¿åº¦å†³å®š",
        "   - å†…å­˜å……è¶³æ—¶å¢å¤§max_kv_cache_len",
        "",
        "3. æ•°å€¼ç¨³å®šæ€§æ”¹è¿›:",
        "   - è€ƒè™‘ä½¿ç”¨bf16è€Œéfp16",
        "   - å¯¹å…³é”®è®¡ç®—ä½¿ç”¨fp32", 
        "   - è°ƒæ•´æ¸©åº¦å‚æ•°ä»¥é¿å…æ•°å€¼æº¢å‡º",
        "",
        "4. è¿›ä¸€æ­¥ä¼˜åŒ–æ–¹å‘:",
        "   - å®ç°grouped-query attention",
        "   - æ·»åŠ rotary position embedding",
        "   - è€ƒè™‘ä½¿ç”¨torch.compileä¼˜åŒ–",
        "   - å®ç°custom CUDA kernels"
    ]
    
    for rec in recommendations:
        print(rec)
    
    return True


if __name__ == "__main__":
    print("å¼€å§‹æ€§èƒ½åˆ†æ...")
    
    try:
        # æ€§èƒ½å¯¹æ¯”
        performance_comparison()
        
        # ç”ŸæˆåŸºå‡†
        generation_benchmark()
        
        # å†…å­˜åˆ†æ  
        memory_analysis()
        
        # ä¼˜åŒ–å»ºè®®
        optimization_recommendations()
        
        print("\n" + "=" * 60)
        print("åˆ†æå®Œæˆï¼")
        print("âœ… Flash Attention + KV Cache æ•´åˆæˆåŠŸ")
        print("ğŸš€ æ¨èåœ¨ç”Ÿæˆä»»åŠ¡ä¸­ä½¿ç”¨KV Cache")
        print("ğŸ’¾ å»ºè®®æ ¹æ®GPUå†…å­˜è°ƒæ•´å—å¤§å°")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
