#!/usr/bin/env python3
"""
ä¿®å¤åçš„Flash Attentioné›†æˆæµ‹è¯•
æ­£ç¡®å¤„ç†KV Cacheçš„ç´¢å¼•é—®é¢˜
"""

import torch
import sys
import os

from config import ModelConfig
from models.mini_llm import create_model
from cache.kv_cache import KVCache
import time


def test_flash_attention_integration():
    """æµ‹è¯•Flash Attentionä¸KV Cacheçš„é›†æˆ"""
    print("=== Flash Attention + KV Cache é›†æˆæµ‹è¯• ===\n")
    
    if not torch.cuda.is_available():
        print("è­¦å‘Š: CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæµ‹è¯•")
        device = "cpu"
        dtype = torch.float32
    else:
        device = "cuda"
        dtype = torch.float16
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    config = ModelConfig(
        dim_model=128,
        num_heads=4,
        num_layers=2,
        vocab_size=1000,
        max_seq_len=256,
        max_kv_cache_len=512,
        max_batch_size=2,
        device=device,
        dtype=dtype
    )
    
    print("æ¨¡å‹é…ç½®:")
    print(f"  dim_model: {config.dim_model}")
    print(f"  num_heads: {config.num_heads}")
    print(f"  head_dim: {config.head_dim}")
    print(f"  num_layers: {config.num_layers}")
    print(f"  è®¾å¤‡: {config.device}")
    print()
    
    # åˆ›å»ºæ¨¡å‹å’Œç¼“å­˜
    model = create_model(config)
    kv_cache = KVCache(config, batch_size=2)
    
    print("âœ“ æ¨¡å‹å’Œç¼“å­˜åˆ›å»ºæˆåŠŸ\n")
    
    # æµ‹è¯•1: å•æ­¥æ¨ç†ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
    print("--- æµ‹è¯•1: å•æ­¥æ¨ç†ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰ ---")
    input_ids = torch.randint(0, config.vocab_size, (2, 16), device=device)
    
    with torch.no_grad():
        logits = model(input_ids, kv_cache=None, use_flash_attention=True)
    
    print(f"âœ“ è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {logits.shape}")
    print()
    
    # æµ‹è¯•2: ä½¿ç”¨KV Cacheçš„æ¨ç†
    print("--- æµ‹è¯•2: ä½¿ç”¨KV Cacheçš„æ¨ç† ---")
    kv_cache.reset()  # é‡ç½®ç¼“å­˜
    
    # ç¬¬ä¸€æ­¥ï¼šè¾“å…¥è¾ƒé•¿çš„åºåˆ—
    prompt = torch.randint(0, config.vocab_size, (2, 20), device=device)
    
    with torch.no_grad():
        logits1 = model(prompt, kv_cache=kv_cache, use_flash_attention=True)
    
    print(f"âœ“ ç¬¬ä¸€æ­¥ - è¾“å…¥: {prompt.shape}, è¾“å‡º: {logits1.shape}")
    print(f"âœ“ ç¼“å­˜é•¿åº¦: {kv_cache.get_seq_len(0)}")
    
    # ç¬¬äºŒæ­¥ï¼šå¢é‡æ¨ç†ï¼ˆåªè¾“å…¥ä¸€ä¸ªtokenï¼‰
    next_token = torch.randint(0, config.vocab_size, (2, 1), device=device)
    
    with torch.no_grad():
        logits2 = model(next_token, kv_cache=kv_cache, use_flash_attention=True)
    
    print(f"âœ“ ç¬¬äºŒæ­¥ - è¾“å…¥: {next_token.shape}, è¾“å‡º: {logits2.shape}")
    print(f"âœ“ æ›´æ–°åç¼“å­˜é•¿åº¦: {kv_cache.get_seq_len(0)}")
    print()
    
    # æµ‹è¯•3: æ€§èƒ½å¯¹æ¯”
    print("--- æµ‹è¯•3: æ€§èƒ½å¯¹æ¯” ---")
    
    if device == "cuda":
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_input = torch.randint(0, config.vocab_size, (2, 32), device=device)
        
        scenarios = [
            ("æ ‡å‡†æ³¨æ„åŠ›", False, None),
            ("Flash Attention", True, None),
        ]
        
        results = {}
        
        for name, use_flash, cache in scenarios:
            print(f"æµ‹è¯• {name}...")
            
            # é¢„çƒ­
            for _ in range(5):
                with torch.no_grad():
                    _ = model(test_input, kv_cache=cache, use_flash_attention=use_flash)
            
            # è®¡æ—¶
            torch.cuda.synchronize()
            start_time = time.time()
            
            num_runs = 50
            for _ in range(num_runs):
                with torch.no_grad():
                    logits = model(test_input, kv_cache=cache, use_flash_attention=use_flash)
            
            torch.cuda.synchronize()
            end_time = time.time()
            
            avg_time = (end_time - start_time) / num_runs * 1000
            throughput = test_input.numel() / (avg_time / 1000)
            
            results[name] = avg_time
            print(f"  å¹³å‡æ—¶é—´: {avg_time:.2f} ms")
            print(f"  ååé‡: {throughput:.0f} tokens/s")
        
        if len(results) >= 2:
            speedup = results["æ ‡å‡†æ³¨æ„åŠ›"] / results["Flash Attention"]
            print(f"  âš¡ Flash AttentionåŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    print()
    
    # æµ‹è¯•4: å¢é‡ç”Ÿæˆæ¨¡æ‹Ÿ
    print("--- æµ‹è¯•4: å¢é‡ç”Ÿæˆæ¨¡æ‹Ÿ ---")
    
    if device == "cuda":
        # é‡ç½®ç¼“å­˜
        kv_cache.reset()
        
        # åˆå§‹prompt
        prompt = torch.randint(0, config.vocab_size, (1, 8), device=device)
        print(f"åˆå§‹prompté•¿åº¦: {prompt.size(1)}")
        
        with torch.no_grad():
            logits = model(prompt, kv_cache=kv_cache, use_flash_attention=True)
            next_token = torch.argmax(logits[:, -1:, :], dim=-1)
        
        print(f"âœ“ å¤„ç†promptå®Œæˆï¼Œç¼“å­˜é•¿åº¦: {kv_cache.get_seq_len(0)}")
        
        # ç”Ÿæˆæ›´å¤štokens
        generation_times = []
        for step in range(5):
            torch.cuda.synchronize()
            start_time = time.time()
            
            with torch.no_grad():
                logits = model(next_token, kv_cache=kv_cache, use_flash_attention=True)
                next_token = torch.argmax(logits[:, -1:, :], dim=-1)
            
            torch.cuda.synchronize()
            end_time = time.time()
            
            step_time = (end_time - start_time) * 1000
            generation_times.append(step_time)
            
            print(f"  æ­¥éª¤ {step + 1}: {step_time:.2f} ms, ç¼“å­˜é•¿åº¦: {kv_cache.get_seq_len(0)}")
        
        avg_gen_time = sum(generation_times) / len(generation_times)
        print(f"  âš¡ å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_gen_time:.2f} ms/token")
        print(f"  âš¡ ç”Ÿæˆé€Ÿåº¦: {1000 / avg_gen_time:.0f} tokens/s")
    
    print("\n=== æ‰€æœ‰æµ‹è¯•å®Œæˆï¼ ===")
    return True


def test_correctness():
    """æµ‹è¯•æ­£ç¡®æ€§ï¼šæ¯”è¾ƒFlash Attentionä¸æ ‡å‡†æ³¨æ„åŠ›çš„è¾“å‡º"""
    print("\n=== æ­£ç¡®æ€§æµ‹è¯• ===\n")
    
    if not torch.cuda.is_available():
        print("è·³è¿‡æ­£ç¡®æ€§æµ‹è¯•ï¼ˆéœ€è¦CUDAï¼‰")
        return
    
    # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä»¥ç¡®ä¿æ•°å€¼ç²¾åº¦
    config = ModelConfig(
        dim_model=64,
        num_heads=4,
        num_layers=1,
        vocab_size=100,
        max_seq_len=128,
        max_kv_cache_len=256,
        max_batch_size=2,
        device="cuda",
        dtype=torch.float32  # ä½¿ç”¨fp32ä»¥è·å¾—æ›´å¥½çš„æ•°å€¼ç²¾åº¦
    )
    
    model = create_model(config)
    
    # å›ºå®šéšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
    torch.manual_seed(42)
    input_ids = torch.randint(0, config.vocab_size, (1, 16), device="cuda")
    
    # æ ‡å‡†æ³¨æ„åŠ›è¾“å‡º
    with torch.no_grad():
        torch.manual_seed(42)  # é‡ç½®éšæœºç§å­
        output_standard = model(input_ids, kv_cache=None, use_flash_attention=False)
    
    # Flash Attentionè¾“å‡º
    with torch.no_grad():
        torch.manual_seed(42)  # é‡ç½®éšæœºç§å­
        output_flash = model(input_ids, kv_cache=None, use_flash_attention=True)
    
    # æ¯”è¾ƒç»“æœ
    max_diff = torch.max(torch.abs(output_standard - output_flash)).item()
    mean_diff = torch.mean(torch.abs(output_standard - output_flash)).item()
    
    print(f"è¾“å‡ºå½¢çŠ¶: {output_standard.shape}")
    print(f"æœ€å¤§å·®å¼‚: {max_diff:.8f}")
    print(f"å¹³å‡å·®å¼‚: {mean_diff:.8f}")
    
    # åˆ¤æ–­æ˜¯å¦é€šè¿‡
    tolerance = 1e-3  # å…è®¸çš„è¯¯å·®èŒƒå›´
    passed = max_diff < tolerance
    
    if passed:
        print(f"âœ“ æ­£ç¡®æ€§æµ‹è¯•é€šè¿‡ (è¯¯å·® < {tolerance})")
    else:
        print(f"âŒ æ­£ç¡®æ€§æµ‹è¯•å¤±è´¥ (è¯¯å·® >= {tolerance})")
    
    return passed


if __name__ == "__main__":
    print("å¼€å§‹Flash Attentioné›†æˆæµ‹è¯•...\n")
    
    try:
        # è¿è¡Œé›†æˆæµ‹è¯•
        integration_success = test_flash_attention_integration()
        
        # è¿è¡Œæ­£ç¡®æ€§æµ‹è¯•
        correctness_success = test_correctness()
        
        print(f"\n{'='*50}")
        print("æµ‹è¯•ç»“æœæ€»ç»“:")
        print(f"  é›†æˆæµ‹è¯•: {'âœ“ é€šè¿‡' if integration_success else 'âŒ å¤±è´¥'}")
        print(f"  æ­£ç¡®æ€§æµ‹è¯•: {'âœ“ é€šè¿‡' if correctness_success else 'âŒ å¤±è´¥'}")
        
        if integration_success and correctness_success:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Flash Attentioné›†æˆæˆåŠŸï¼")
        else:
            print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
