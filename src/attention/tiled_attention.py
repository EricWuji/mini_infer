import torch
import torch.nn.functional as F
from typing import Tuple
import math
import time


def standard_attention(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:
    """
    ç”¨äºæ¯”è¾ƒçš„æ ‡å‡†æ³¨æ„åŠ›å®ç°
    """
    scale = q.size(-1) ** -0.5
    scores = (q @ k.T) * scale
    attn_weights = F.softmax(scores, dim=-1)
    output = attn_weights @ v
    return output

def tiled_attention(
    q: torch.Tensor, # [seq_len, dim] æŸ¥è¯¢çŸ©é˜µ
    k: torch.Tensor, # [seq_len, dim] é”®çŸ©é˜µ
    v: torch.Tensor, # [seq_len, dim] å€¼çŸ©é˜µ
    block_size: int = 128
) -> torch.Tensor:
    """
    Flash Attention 2 å®ç°ï¼ŒåŒ…å«åˆ†å—ï¼ˆtilingï¼‰å’Œåœ¨çº¿ softmaxï¼ˆonline softmaxï¼‰
    
    è¯¥å®ç°é‡‡ç”¨äº† Flash Attention çš„å…³é”®æ€æƒ³ï¼š
    1. åˆ†å—ï¼ˆTilingï¼‰ï¼šå°†è®¡ç®—åˆ’åˆ†ä¸ºå¤šä¸ªå—ï¼Œä»¥å‡å°‘å†…å­˜ä½¿ç”¨
    2. åœ¨çº¿ Softmaxï¼ˆOnline Softmaxï¼‰ï¼šå¢é‡å¼è®¡ç®— softmax ç»Ÿè®¡é‡ï¼Œé¿å…å­˜å‚¨å¤§å‹ä¸­é—´çŸ©é˜µ
    3. é‡è®¡ç®—ï¼ˆRecomputationï¼‰ï¼šé€šè¿‡é‡æ–°è®¡ç®—æ³¨æ„åŠ›æƒé‡æ¥æ¢å–å†…å­˜ï¼Œè€Œéå­˜å‚¨å®ƒä»¬
    
    ç›¸æ¯”æœ´ç´ æ³¨æ„åŠ›çš„ä¸»è¦æ”¹è¿›ï¼š
    - å†…å­˜å¤æ‚åº¦ä» O(NÂ²) é™ä½è‡³ O(N)
    - é€šè¿‡åœ¨çº¿è¿½è¸ªæœ€å¤§å€¼ä¿è¯æ•°å€¼ç¨³å®šæ€§
    - é€šè¿‡é¡ºåºå¤„ç†å—å®ç°å†…å­˜é«˜æ•ˆ
    
    Args:
        q, k, v: [seq_len, dim] - æŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰ã€å€¼ï¼ˆValueï¼‰çŸ©é˜µ
        block_size: int - æ¯ä¸ªå—çš„å¤§å°ï¼ˆåœ¨å†…å­˜å’Œè®¡ç®—ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼‰
        
    Return:
        output: [seq_len, dim] - æ³¨æ„åŠ›è¾“å‡º
        
    ç®—æ³•ç»†èŠ‚ï¼š
    - å¯¹äºæ¯ä¸ªæŸ¥è¯¢å— Q_iï¼Œéå†æ‰€æœ‰é”®å€¼å— K_j, V_j
    - ç»´æŠ¤è¿è¡Œæ—¶ç»Ÿè®¡é‡ï¼ˆæœ€å¤§å€¼ m_i å’Œæ€»å’Œ l_iï¼‰ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
    - ä½¿ç”¨åœ¨çº¿ softmax æ›´æ–°ï¼Œé¿å…å­˜å‚¨å®Œæ•´çš„æ³¨æ„åŠ›çŸ©é˜µ
    - æœ€ç»ˆçš„å½’ä¸€åŒ–ç¡®ä¿ softmax è®¡ç®—æ­£ç¡®
    
    å¼ é‡å½¢çŠ¶è¯´æ˜ï¼š
    - q_i: [min(block_size, seq_len-i), dim] - å½“å‰æŸ¥è¯¢å—
    - k_j: [min(block_size, seq_len-j), dim] - å½“å‰é”®å—
    - v_j: [min(block_size, seq_len-j), dim] - å½“å‰å€¼å—
    - s_ij: [min(block_size, seq_len-i), min(block_size, seq_len-j)] - æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ
    - o_i: [min(block_size, seq_len-i), dim] - å½“å‰æŸ¥è¯¢å—çš„è¾“å‡º
    - l_i: [min(block_size, seq_len-i)] - å½“å‰æŸ¥è¯¢å—çš„è¡Œå’Œ
    - m_i: [min(block_size, seq_len-i)] - å½“å‰æŸ¥è¯¢å—çš„è¡Œæœ€å¤§å€¼
    - p_ij: [min(block_size, seq_len-i), min(block_size, seq_len-j)] - æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
    """

    seq_len, dim = q.shape
    scale = dim ** -0.5
    o = torch.zeros_like(q, dtype=torch.float32)  # ä½¿ç”¨float32æé«˜æ•°å€¼ç¨³å®šæ€§
    l = torch.zeros(seq_len, device=q.device, dtype=torch.float32)  # è¡Œå’Œ
    m = torch.full((seq_len,), -torch.inf, device=q.device, dtype=torch.float32)  # è¡Œæœ€å¤§å€¼

    # å°†è¾“å…¥è½¬æ¢ä¸ºfloat32è¿›è¡Œè®¡ç®—
    q = q.float()
    k = k.float()
    v = v.float()

    # æŸ¥è¯¢å—çš„å¤–å¾ªç¯
    for i in range(0, seq_len, block_size):
        q_i = q[i:i + block_size] * scale  # [block_size, dim]
        o_i = torch.zeros(min(block_size, seq_len - i), dim, device=q.device, dtype=torch.float32)
        l_i = torch.zeros(min(block_size, seq_len - i), device=q.device, dtype=torch.float32)
        m_i = torch.full((min(block_size, seq_len - i),), -torch.inf, device=q.device, dtype=torch.float32)
        
        # é”®å€¼å—çš„å†…å¾ªç¯
        for j in range(0, seq_len, block_size):
            k_j = k[j:j + block_size]  # [block_size, dim]
            v_j = v[j:j + block_size]  # [block_size, dim]
            
            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
            s_ij = q_i @ k_j.T  # [q_block_size, k_block_size]
            
            # åœ¨çº¿softmaxæ›´æ–° - Flash Attentionçš„å…³é”®æ€æƒ³
            m_ij = s_ij.max(dim=-1)[0]  # [q_block_size] - å½“å‰å—çš„æœ€å¤§å€¼
            
            # æ›´æ–°è¡Œæœ€å¤§å€¼
            m_i_new = torch.maximum(m_i, m_ij)
            
            # è®¡ç®—å…·æœ‰æ•°å€¼ç¨³å®šæ€§çš„æŒ‡æ•°å€¼
            alpha = torch.exp(m_i - m_i_new)  # [q_block_size] - ä¹‹å‰å—çš„é‡æ–°ç¼©æ”¾å› å­
            beta = torch.exp(m_ij - m_i_new)   # [q_block_size] - æœªä½¿ç”¨ä½†ä¿ç•™ä»¥ä¾¿ç†è§£

            
            # æ›´æ–°è¾“å‡ºå’Œå½’ä¸€åŒ–
            o_i = o_i * alpha.unsqueeze(-1)  # é‡æ–°ç¼©æ”¾ä¹‹å‰çš„è¾“å‡º
            
            # è®¡ç®—å½“å‰å—çš„æ³¨æ„åŠ›æƒé‡
            p_ij = torch.exp(s_ij - m_i_new.unsqueeze(-1))  # [q_block_size, k_block_size]
            
            # ç”¨å½“å‰å—çš„è´¡çŒ®æ›´æ–°è¾“å‡º
            o_i = o_i + (p_ij @ v_j)  # [q_block_size, dim]
            
            # æ›´æ–°å½’ä¸€åŒ–å› å­
            l_i = l_i * alpha + p_ij.sum(dim=-1)
            m_i = m_i_new
        
        # å­˜å‚¨å¸¦æœ‰æœ€ç»ˆå½’ä¸€åŒ–çš„ç»“æœ
        actual_block_size = min(block_size, seq_len - i)
        o[i:i + actual_block_size] = o_i / l_i.unsqueeze(-1)
        l[i:i + actual_block_size] = l_i
        m[i:i + actual_block_size] = m_i

    return o.to(q.dtype)


def test_flash_attention():
    """
    éªŒè¯Flash Attentionå®ç°çš„æµ‹è¯•å‡½æ•°
    """
    torch.manual_seed(42)
    
    print("æµ‹è¯•Flash Attention 2å®ç°")
    print("=" * 50)
    
    # ä½¿ç”¨ä¸åŒé…ç½®è¿›è¡Œæµ‹è¯•
    test_configs = [
        {"seq_len": 128, "dim": 32, "block_size": 32},
        {"seq_len": 256, "dim": 64, "block_size": 64},
        {"seq_len": 512, "dim": 128, "block_size": 128},
        {"seq_len": 1024, "dim": 64, "block_size": 256},
    ]
    
    all_passed = True
    
    for i, config in enumerate(test_configs):
        print(f"\næµ‹è¯• {i+1}: seq_len={config['seq_len']}, dim={config['dim']}, block_size={config['block_size']}")
        
        # ç”Ÿæˆéšæœºè¾“å…¥
        q = torch.randn(config["seq_len"], config["dim"])
        k = torch.randn(config["seq_len"], config["dim"])
        v = torch.randn(config["seq_len"], config["dim"])
        
        # è®¡ç®—è¾“å‡º
        flash_output = tiled_attention(q, k, v, block_size=config["block_size"])
        standard_output = standard_attention(q, k, v)
        
        # æ£€æŸ¥æ•°å€¼ç²¾åº¦
        max_diff = torch.max(torch.abs(flash_output - standard_output)).item()
        mean_diff = torch.mean(torch.abs(flash_output - standard_output)).item()
        
        print(f"  æœ€å¤§å·®å¼‚: {max_diff:.8f}")
        print(f"  å¹³å‡å·®å¼‚: {mean_diff:.8f}")
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦æ¥è¿‘
        is_close = torch.allclose(flash_output, standard_output, atol=1e-4, rtol=1e-4)
        
        if is_close:
            print(f"  âœ… æµ‹è¯• {i+1} é€šè¿‡!")
        else:
            print(f"  âŒ æµ‹è¯• {i+1} å¤±è´¥!")
            all_passed = False
    
    print("\n" + "=" * 50)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰Flash Attentionæµ‹è¯•éƒ½é€šè¿‡äº†!")
    else:
        print("ğŸ’¥ éƒ¨åˆ†æµ‹è¯•å¤±è´¥!")
        
    return all_passed


def benchmark_memory_usage():
    """
    æ¯”è¾ƒæ ‡å‡†æ³¨æ„åŠ›å’Œflash attentionçš„å†…å­˜ä½¿ç”¨æƒ…å†µåŸºå‡†æµ‹è¯•
    """
    import time
    import gc
    
    print("\nå†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•")
    print("=" * 40)
    
    # æµ‹è¯•é…ç½® - é€’å¢çš„åºåˆ—é•¿åº¦
    configs = [512, 1024, 2048]  # seq_lens
    dim = 128
    block_size = 128
    
    for seq_len in configs:
        print(f"\nåºåˆ—é•¿åº¦: {seq_len}, ç»´åº¦: {dim}")
        print("-" * 30)
        
        # ç”Ÿæˆæ•°æ®
        q = torch.randn(seq_len, dim)
        k = torch.randn(seq_len, dim)  
        v = torch.randn(seq_len, dim)
        
        # æ¸…ç©ºç¼“å­˜
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        gc.collect()
        
        # è®¡æ—¶Flash Attention
        torch.manual_seed(42)
        start_time = time.time()
        flash_output = tiled_attention(q, k, v, block_size=block_size)
        flash_time = time.time() - start_time
        
        # è®¡æ—¶æ ‡å‡†æ³¨æ„åŠ›
        torch.manual_seed(42)
        start_time = time.time()
        standard_output = standard_attention(q, k, v)
        standard_time = time.time() - start_time
        
        # éªŒè¯æ­£ç¡®æ€§
        max_diff = torch.max(torch.abs(flash_output - standard_output)).item()
        
        print(f"Flash Attentionæ—¶é—´:    {flash_time:.4f}s")
        print(f"æ ‡å‡†æ³¨æ„åŠ›æ—¶é—´: {standard_time:.4f}s")
        print(f"é€Ÿåº¦æ¯”: {standard_time/flash_time:.2f}x")
        print(f"æœ€å¤§å·®å¼‚: {max_diff:.2e}")
        
        # å†…å­˜ä½¿ç”¨ä¼°ç®—
        standard_memory = seq_len * seq_len * 4  # æ³¨æ„åŠ›çŸ©é˜µçš„å­—èŠ‚æ•°
        flash_memory = block_size * block_size * 4  # æ³¨æ„åŠ›å—çš„å­—èŠ‚æ•°
        memory_savings = (standard_memory - flash_memory) / standard_memory * 100
        
        print(f"é¢„è®¡å†…å­˜èŠ‚çœ: {memory_savings:.1f}%")
        print(f"æ ‡å‡†å†…å­˜ï¼ˆæ³¨æ„åŠ›çŸ©é˜µï¼‰: {standard_memory/1024/1024:.1f} MB")
        print(f"Flashå†…å­˜ï¼ˆæ³¨æ„åŠ›å—ï¼‰: {flash_memory/1024:.1f} KB")


def demo_usage():
    """
    æ¼”ç¤ºFlash Attentionå®ç°çš„å…¸å‹ç”¨æ³•
    """
    print("\nFlash Attention 2ä½¿ç”¨æ¼”ç¤º")
    print("=" * 40)
    
    # ä¾‹å­ï¼šæ¨¡æ‹Ÿä¸€ä¸ªå°å‹transformeræ³¨æ„åŠ›å±‚
    batch_size = 4
    seq_len = 512
    dim = 128
    num_heads = 8
    head_dim = dim // num_heads
    
    print(f"æ¨¡æ‹Ÿå¤šå¤´æ³¨æ„åŠ›:")
    print(f"- æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"- åºåˆ—é•¿åº¦: {seq_len}")
    print(f"- éšè—ç»´åº¦: {dim}")  
    print(f"- å¤´æ•°: {num_heads}")
    print(f"- å¤´ç»´åº¦: {head_dim}")
    
    # ç”Ÿæˆéšæœºè¾“å…¥ï¼ˆé€šå¸¸è¿™äº›æ¥è‡ªåµŒå…¥ï¼‰
    x = torch.randn(batch_size, seq_len, dim)
    
    # æ¨¡æ‹Ÿå¤šå¤´æ³¨æ„åŠ›è®¡ç®—
    total_time = 0
    
    for b in range(batch_size):
        for h in range(num_heads):
            # æå–ç‰¹å®šå¤´çš„Q, K, Vï¼ˆç®€åŒ–ç‰ˆ - é€šå¸¸æ¥è‡ªçº¿æ€§æŠ•å½±ï¼‰
            start_dim = h * head_dim
            end_dim = start_dim + head_dim
            
            q = x[b, :, start_dim:end_dim]  # [seq_len, head_dim]
            k = x[b, :, start_dim:end_dim]  # [seq_len, head_dim]  
            v = x[b, :, start_dim:end_dim]  # [seq_len, head_dim]
            
            # åº”ç”¨Flash Attention
            start_time = time.time()
            output = tiled_attention(q, k, v, block_size=64)
            total_time += time.time() - start_time
    
    print(f"\næ€»è®¡ç®—æ—¶é—´: {total_time:.4f}s")
    print(f"æ¯ä¸ªå¤´çš„å¹³å‡æ—¶é—´: {total_time/(batch_size*num_heads):.4f}s")
    print("âœ… å¤šå¤´æ³¨æ„åŠ›æ¨¡æ‹Ÿå®Œæˆ!")


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•
    test_flash_attention()
    # benchmark_memory_usage()
    # demo_usage()
