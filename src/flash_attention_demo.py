"""
Flash Attention 2 + KV Cache å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨MiniLLMä¸­ä½¿ç”¨Tritonå®ç°çš„Flash Attention 2
ç»“åˆKV Cacheæ¥å®ç°é«˜æ•ˆçš„æ–‡æœ¬ç”Ÿæˆ
"""

import torch

from config import ModelConfig
from models.mini_llm import create_model
from cache.kv_cache import KVCache
import time


def create_sample_model():
    """åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ¨¡å‹é…ç½®"""
    config = ModelConfig(
        dim_model=256,        # æ¨¡å‹ç»´åº¦
        num_heads=8,          # æ³¨æ„åŠ›å¤´æ•°
        num_layers=4,         # å±‚æ•°
        vocab_size=10000,     # è¯æ±‡è¡¨å¤§å°
        max_seq_len=512,      # æœ€å¤§åºåˆ—é•¿åº¦
        max_kv_cache_len=1024,# KVç¼“å­˜æœ€å¤§é•¿åº¦
        max_batch_size=4,     # æ‰¹å¤§å°
        device="cuda" if torch.cuda.is_available() else "cpu",
        dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    
    return create_model(config), config


def simulate_text_generation(model, config, prompt_length=10, generate_length=20):
    """æ¨¡æ‹Ÿæ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹"""
    print(f"=== æ¨¡æ‹Ÿæ–‡æœ¬ç”Ÿæˆ (prompt: {prompt_length}, generate: {generate_length}) ===")
    
    # åˆ›å»ºKV Cache
    kv_cache = KVCache(config, batch_size=1)
    device = config.device
    
    # ç”Ÿæˆéšæœºprompt
    prompt = torch.randint(0, config.vocab_size, (1, prompt_length), device=device)
    print(f"åˆå§‹prompté•¿åº¦: {prompt_length}")
    
    # ç¬¬ä¸€æ­¥ï¼šå¤„ç†prompt
    start_time = time.time()
    
    with torch.no_grad():
        logits = model(prompt, kv_cache=kv_cache, use_flash_attention=True)
        next_token_id = torch.argmax(logits[:, -1:, :], dim=-1)
    
    prompt_time = (time.time() - start_time) * 1000
    print(f"Promptå¤„ç†æ—¶é—´: {prompt_time:.2f} ms")
    print(f"ç¼“å­˜é•¿åº¦: {kv_cache.get_seq_len(0)}")
    
    # å¢é‡ç”Ÿæˆ
    generated_tokens = []
    generation_times = []
    
    print("\nå¼€å§‹å¢é‡ç”Ÿæˆ...")
    for step in range(generate_length):
        torch.cuda.synchronize() if device == "cuda" else None
        step_start = time.time()
        
        with torch.no_grad():
            # åªè¾“å…¥ä¸€ä¸ªæ–°token
            logits = model(next_token_id, kv_cache=kv_cache, use_flash_attention=True)
            next_token_id = torch.argmax(logits[:, -1:, :], dim=-1)
        
        torch.cuda.synchronize() if device == "cuda" else None
        step_time = (time.time() - step_start) * 1000
        
        generation_times.append(step_time)
        generated_tokens.append(next_token_id.item())
        
        if step < 5 or step % 5 == 4:  # åªæ˜¾ç¤ºå‰å‡ æ­¥å’Œæ¯5æ­¥çš„ç»“æœ
            print(f"  æ­¥éª¤ {step + 1:2d}: {step_time:5.2f} ms, token: {next_token_id.item():4d}, ç¼“å­˜: {kv_cache.get_seq_len(0):3d}")
    
    # ç»Ÿè®¡ç»“æœ
    avg_generation_time = sum(generation_times) / len(generation_times)
    total_time = prompt_time + sum(generation_times)
    total_tokens = prompt_length + generate_length
    
    print(f"\nç”Ÿæˆå®Œæˆ!")
    print(f"  å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_generation_time:.2f} ms/token")
    print(f"  æ€»æ—¶é—´: {total_time:.2f} ms")
    print(f"  æ€»tokens: {total_tokens}")
    print(f"  æ€»ä½“é€Ÿåº¦: {total_tokens / (total_time / 1000):.1f} tokens/s")
    print(f"  å¢é‡é€Ÿåº¦: {1000 / avg_generation_time:.1f} tokens/s")
    
    return generated_tokens


def benchmark_different_configurations():
    """æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½"""
    print("\n=== ä¸åŒé…ç½®æ€§èƒ½æµ‹è¯• ===")
    
    if not torch.cuda.is_available():
        print("éœ€è¦CUDAæ‰èƒ½è¿›è¡Œæ€§èƒ½æµ‹è¯•")
        return
    
    configurations = [
        ("å°æ¨¡å‹", ModelConfig(dim_model=128, num_heads=4, num_layers=2, vocab_size=5000)),
        ("ä¸­æ¨¡å‹", ModelConfig(dim_model=256, num_heads=8, num_layers=4, vocab_size=10000)),
        ("å¤§æ¨¡å‹", ModelConfig(dim_model=512, num_heads=16, num_layers=6, vocab_size=20000))
    ]
    
    test_sequence_length = 64
    
    for config_name, config in configurations:
        print(f"\n--- {config_name} ---")
        print(f"å‚æ•°: dim={config.dim_model}, heads={config.num_heads}, layers={config.num_layers}")
        
        # åˆ›å»ºæ¨¡å‹
        model = create_model(config)
        param_count = sum(p.numel() for p in model.parameters())
        print(f"å‚æ•°æ•°é‡: {param_count:,}")
        
        # æµ‹è¯•è¾“å…¥
        input_ids = torch.randint(0, config.vocab_size, (2, test_sequence_length), device="cuda")
        
        # æµ‹è¯•æ ‡å‡†æ³¨æ„åŠ›
        model.train(False)
        with torch.no_grad():
            # é¢„çƒ­
            for _ in range(3):
                _ = model(input_ids, use_flash_attention=False)
            
            # è®¡æ—¶
            torch.cuda.synchronize()
            start = time.time()
            for _ in range(10):
                logits_standard = model(input_ids, use_flash_attention=False)
            torch.cuda.synchronize()
            time_standard = (time.time() - start) / 10 * 1000
        
        # æµ‹è¯•Flash Attention
        with torch.no_grad():
            # é¢„çƒ­
            for _ in range(3):
                _ = model(input_ids, use_flash_attention=True)
            
            # è®¡æ—¶
            torch.cuda.synchronize()
            start = time.time()
            for _ in range(10):
                logits_flash = model(input_ids, use_flash_attention=True)
            torch.cuda.synchronize()
            time_flash = (time.time() - start) / 10 * 1000
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = time_standard / time_flash
        throughput_standard = input_ids.numel() / (time_standard / 1000)
        throughput_flash = input_ids.numel() / (time_flash / 1000)
        
        print(f"æ ‡å‡†æ³¨æ„åŠ›: {time_standard:.2f} ms ({throughput_standard:.0f} tokens/s)")
        print(f"Flashæ³¨æ„åŠ›: {time_flash:.2f} ms ({throughput_flash:.0f} tokens/s)")
        print(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")


def demonstrate_batch_processing():
    """æ¼”ç¤ºæ‰¹å¤„ç†èƒ½åŠ›"""
    print("\n=== æ‰¹å¤„ç†æ¼”ç¤º ===")
    
    if not torch.cuda.is_available():
        print("éœ€è¦CUDAæ‰èƒ½è¿›è¡Œæ‰¹å¤„ç†æµ‹è¯•")
        return
    
    config = ModelConfig(
        dim_model=256,
        num_heads=8, 
        num_layers=3,
        vocab_size=8000,
        max_batch_size=8,
        device="cuda",
        dtype=torch.float16
    )
    
    model = create_model(config)
    
    batch_sizes = [1, 2, 4, 8]
    seq_len = 32
    
    print(f"æµ‹è¯•åºåˆ—é•¿åº¦: {seq_len}")
    print("æ‰¹å¤§å° | æ—¶é—´(ms) | ååé‡(tokens/s) | æ¯æ ·æœ¬æ—¶é—´(ms)")
    print("-" * 55)
    
    for batch_size in batch_sizes:
        input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len), device="cuda")
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(3):
                _ = model(input_ids, use_flash_attention=True)
        
        # è®¡æ—¶
        torch.cuda.synchronize()
        start = time.time()
        
        num_runs = 20
        with torch.no_grad():
            for _ in range(num_runs):
                logits = model(input_ids, use_flash_attention=True)
        
        torch.cuda.synchronize()
        total_time = (time.time() - start) / num_runs * 1000
        
        throughput = input_ids.numel() / (total_time / 1000)
        per_sample_time = total_time / batch_size
        
        print(f"{batch_size:7d} | {total_time:7.2f} | {throughput:13.0f} | {per_sample_time:12.2f}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Flash Attention 2 + KV Cache å®Œæ•´ä½¿ç”¨ç¤ºä¾‹\n")
    
    # æ£€æŸ¥ç¯å¢ƒ
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAè®¾å¤‡: {torch.cuda.get_device_name()}")
    print()
    
    # åˆ›å»ºç¤ºä¾‹æ¨¡å‹
    print("åˆ›å»ºç¤ºä¾‹æ¨¡å‹...")
    model, config = create_sample_model()
    
    param_count = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {param_count:,}")
    print(f"æ¨¡å‹é…ç½®: {config.dim_model}d, {config.num_heads}h, {config.num_layers}L")
    print()
    
    # 1. æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º
    simulate_text_generation(model, config, prompt_length=15, generate_length=25)
    
    # 2. æ€§èƒ½åŸºå‡†æµ‹è¯•
    benchmark_different_configurations()
    
    # 3. æ‰¹å¤„ç†æ¼”ç¤º
    demonstrate_batch_processing()
    
    print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“ æ€»ç»“:")
    print("1. âœ… Flash Attention 2 Triton kernelæˆåŠŸé›†æˆåˆ°MiniLLM")
    print("2. âœ… KV Cacheä¸Flash AttentionååŒå·¥ä½œ")
    print("3. âœ… æ”¯æŒé«˜æ•ˆçš„å¢é‡ç”Ÿæˆ")
    print("4. âœ… æ”¯æŒæ‰¹å¤„ç†æ¨ç†")
    print("5. âœ… ç›¸æ¯”æ ‡å‡†æ³¨æ„åŠ›æœ‰æ€§èƒ½æå‡")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        print(f"\nâŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
